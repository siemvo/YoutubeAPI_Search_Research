{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ca194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your API key in a .txt file in this folder\n",
    "with open(\"API_KEY.txt\", \"r\") as file:\n",
    "    API_KEY = file.read().strip()\n",
    "\n",
    "#pip install --upgrade google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd1fedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created CSV file: video_data.csv\n",
      "Loaded 0 existing video IDs.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'isodate'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 113\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[32m    112\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33m2hollis type beat\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m _ = \u001b[43mcollect_video_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mcollect_video_data\u001b[39m\u001b[34m(query, max_results, csv_file)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# Get duration in seconds\u001b[39;00m\n\u001b[32m     69\u001b[39m duration_iso = video_info[\u001b[33m'\u001b[39m\u001b[33mcontentDetails\u001b[39m\u001b[33m'\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33mduration\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mPT0S\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m duration_seconds = \u001b[43mparse_duration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mduration_iso\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# Get the highest quality thumbnail available\u001b[39;00m\n\u001b[32m     73\u001b[39m thumbnail_info = video_info[\u001b[33m'\u001b[39m\u001b[33msnippet\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mthumbnails\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mparse_duration\u001b[39m\u001b[34m(duration)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_duration\u001b[39m(duration):\n\u001b[32m     27\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Convert ISO 8601 duration to seconds.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01misodate\u001b[39;00m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     30\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(isodate.parse_duration(duration).total_seconds())\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'isodate'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "CSV_FILE = \"video_data.csv\"\n",
    "\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "def initialize_csv(csv_file):\n",
    "    if not os.path.exists(csv_file):\n",
    "        with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=[\"video_id\", \"title\", \"views\", \"tags\", \"channel_name\", \"subscribers\", \"search_query\", \"thumbnail_url\"])\n",
    "            writer.writeheader()\n",
    "        print(f\"Created CSV file: {csv_file}\")\n",
    "    else:\n",
    "        print(f\"CSV file already exists: {csv_file}\")\n",
    "\n",
    "def load_existing_video_ids(csv_file):\n",
    "    if not os.path.exists(csv_file):\n",
    "        return set()\n",
    "\n",
    "    with open(csv_file, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        return {row[\"video_id\"] for row in reader}\n",
    "\n",
    "def collect_video_data(query, max_results=100, csv_file=CSV_FILE):\n",
    "    \"\"\"Collect video data from YouTube API with pagination.\"\"\"\n",
    "    initialize_csv(csv_file)\n",
    "\n",
    "    existing_ids = load_existing_video_ids(csv_file)\n",
    "    print(f\"Loaded {len(existing_ids)} existing video IDs.\")\n",
    "\n",
    "    video_data = []\n",
    "    count = 0\n",
    "    next_page_token = None\n",
    "\n",
    "    while count < max_results:\n",
    "        search_request = youtube.search().list(q=query, part=\"id,snippet\", maxResults=50, pageToken=next_page_token)\n",
    "        search_response = search_request.execute()\n",
    "        for item in search_response['items']:\n",
    "            # Check if the item is a video (not a playlist or channel)\n",
    "            if item['id']['kind'] != 'youtube#video':\n",
    "                continue\n",
    "\n",
    "            video_id = item['id']['videoId']\n",
    "            \n",
    "            # Skip already collected videos\n",
    "            if video_id in existing_ids:\n",
    "                continue\n",
    "            \n",
    "            # Get the video information\n",
    "            video_request = youtube.videos().list(part=\"snippet,statistics\", id=video_id)\n",
    "            video_response = video_request.execute()\n",
    "            \n",
    "            video_info = video_response['items'][0]\n",
    "            title = video_info['snippet']['title']\n",
    "            views = video_info['statistics'].get('viewCount', 'Unknown')\n",
    "            tags = video_info['snippet'].get('tags', [])\n",
    "            channel_name = video_info['snippet']['channelTitle']\n",
    "            \n",
    "            # Get the highest quality thumbnail available (maxres -> high -> standard -> default)\n",
    "            thumbnail_info = video_info['snippet']['thumbnails']\n",
    "            thumbnail_url = thumbnail_info.get('maxres', thumbnail_info.get('high', thumbnail_info.get('standard', thumbnail_info.get('default')))).get('url')\n",
    "\n",
    "            # Get channel subscribers\n",
    "            channel_request = youtube.channels().list(part=\"statistics\", id=video_info['snippet']['channelId'])\n",
    "            channel_response = channel_request.execute()\n",
    "            subscribers = channel_response['items'][0]['statistics'].get('subscriberCount', 'Unknown')\n",
    "\n",
    "            # Append data\n",
    "            video_data.append({\n",
    "                \"video_id\": video_id,\n",
    "                \"title\": title,\n",
    "                \"views\": views,\n",
    "                \"tags\": \"|\".join(tags),  # Join tags as a string\n",
    "                \"channel_name\": channel_name,\n",
    "                \"subscribers\": subscribers,\n",
    "                \"search_query\": query,\n",
    "                \"thumbnail_url\": thumbnail_url\n",
    "            })\n",
    "\n",
    "            count += 1\n",
    "            if count >= max_results:\n",
    "                break\n",
    "\n",
    "        # Check for next page\n",
    "        next_page_token = search_response.get('nextPageToken', None)\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    # Save the data to CSV\n",
    "    with open(csv_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"video_id\", \"title\", \"views\", \"tags\", \"channel_name\", \"subscribers\", \"search_query\", \"thumbnail_url\"])\n",
    "        writer.writerows(video_data)\n",
    "\n",
    "    print(f\"\\nData saved to {csv_file}. New videos collected: {len(video_data)}\")\n",
    "    return video_data\n",
    "\n",
    "# Parameters to change\n",
    "query = \"2hollis type beat\"\n",
    "_ = collect_video_data(query, max_results=50)  # Change max_results (this uses credits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d892d9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV updated. 300 rows processed.\n"
     ]
    }
   ],
   "source": [
    "# Download thumbnails (skips already downloaded ones)\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Folder where thumbnails will be saved\n",
    "THUMBNAIL_DIR = \"thumbnails\"\n",
    "\n",
    "def download_thumbnail(thumbnail_url, video_id):\n",
    "    \"\"\"Download the thumbnail and return the local file path.\"\"\"\n",
    "    if not thumbnail_url:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(THUMBNAIL_DIR):\n",
    "            os.makedirs(THUMBNAIL_DIR)\n",
    "\n",
    "        # Request to download the thumbnail given url\n",
    "        response = requests.get(thumbnail_url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        file_path = os.path.join(THUMBNAIL_DIR, f\"{video_id}.jpg\")\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        return file_path # To reference in csv\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading thumbnail for video {video_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def update_thumbnails_in_csv(csv_file=\"video_data.csv\"):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Check if the 'thumbnail_path' column exists otherwise create it\n",
    "    if 'thumbnail_path' not in df.columns:\n",
    "        df['thumbnail_path'] = None\n",
    "\n",
    "    # Loop through rows and update the thumbnail if missing\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row['thumbnail_path']):  # If 'thumbnail_path' is missing\n",
    "            local_thumbnail_path = download_thumbnail(row['thumbnail_url'], row['video_id'])\n",
    "            if local_thumbnail_path:\n",
    "                df.at[index, 'thumbnail_path'] = local_thumbnail_path  # Update the path in the DataFrame\n",
    "\n",
    "    # Save the updated DataFrame back to the CSV\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"CSV updated. {len(df)} rows processed.\")\n",
    "\n",
    "update_thumbnails_in_csv(\"video_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4a0a4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gebruiker\\AppData\\Local\\Temp\\ipykernel_41552\\2878370686.py:10: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  data_new = data_new[~data['title'].str.lower().str.contains('|'.join(strings_to_remove), case=False, na=False)]\n"
     ]
    }
   ],
   "source": [
    "# Remove tutorials and add search volume\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"video_data.csv\")\n",
    "\n",
    "data_new = data.sort_values(by=\"views\", ascending=False) # Highest views first\n",
    "\n",
    "strings_to_remove = [\"how to\", \"tutorial\", \"how\"]\n",
    "data_new = data_new[~data['title'].str.lower().str.contains('|'.join(strings_to_remove), case=False, na=False)]\n",
    "\n",
    "# Search term volume\n",
    "search_volume_dict = {\n",
    "    \"2hollis type beat\": 11237409,\n",
    "    \"nate sib type beat\": 261955\n",
    "}\n",
    "\n",
    "# Add the search_volume column based on the search_query column\n",
    "data_new[\"search_volume\"] = data_new[\"search_query\"].str.lower().map(search_volume_dict).fillna(0)\n",
    "\n",
    "data_new.to_csv(\"data_final.csv\", index=False, encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
